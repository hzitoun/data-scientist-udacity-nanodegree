{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to learn using gradient descent** error function has to be continuous and differentiable.\n",
    "- Sigmoid function $sigmoid(x) = \\frac{1}{1+e^{-x}}$ gives not discrete value but probability of being a value (probabilty space). Sigmoid activation function is used when we have 2 labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Class Classification and Softmax**\n",
    "\n",
    "- The softmax function is the equivalent of the sigmoid activation function but when the problem has 3 or more classes.\n",
    "- We caclulate a score for each label then we compute the probability by using $exp$ (it always give a positive number) so that our probability will always be positive\n",
    "- Let us say we have N classes and a linear model that gives us the following scores Z1, Z2....ZN:\n",
    "   $$P(class i) = \\frac{e^{Zi}}{e^{Z1} + e^{Z2} + ... + e^{ZN}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00669285, 0.99330715])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    return np.exp(L) / (np.sum(np.exp(L)))\n",
    "\n",
    "softmax([0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding \n",
    "\n",
    "If input data is not numbers, we need to encode it to numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can apply both transformations (from text categories to integer categories, \n",
    "#                                    then from integer categories to one-hot vectors)\n",
    "# in one shot using the LabelBinarizer class:\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "cat_features = ['color', 'director_name', 'actor_2_name']\n",
    "encoder = LabelBinarizer()\n",
    "new_cat_features = encoder.fit_transform(cat_features)\n",
    "new_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_Cat', 'x0_Dog', 'x0_Goat', 'x0_Mouse', 'x1_1', 'x1_2', 'x1_3',\n",
       "       'x1_5', 'x2_1', 'x2_2', 'x2_3', 'x2_5'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X = np.array([['Cat', 1, 1], ['Dog', 3, 3,], ['Mouse', 2, 2], ['Goat', 5, 5]])\n",
    "\n",
    "oneHotEncoder  = OneHotEncoder()\n",
    "\n",
    "oneHotEncoder.fit_transform(X)\n",
    "\n",
    "oneHotEncoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "Probability will be one of our best friends as we go through Deep Learning. In this lesson, we'll see how we can use probability to evaluate (and improve!) our models.\n",
    "\n",
    "We pick the model that gives the existing labels the highest probability, thus by maximizing the probability we can pick the best possible model.\n",
    "\n",
    "The model classifies most points correctly with P(all) indicating how accurate the model is.\n",
    "\n",
    "$$\n",
    "P (all) = P(e1) * P(e2) * P(e3) * ... * P(en)\n",
    "$$\n",
    "\n",
    "All need to do is to maximize this probability. We are not going to maximize the product since if we have thousands of points (where each prob is 0 <  < 1) the product will be very tiny 0.00000000000000000000 something. So we gonna do sum instead. To turn the product to sum we gonna use log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "ln base e \n",
    "$$ln (P) = ln(P1 .P2 . P3 .P4) = ln(P1) + ln(P2) + ln(P3) + ln(P4) $$ \n",
    "\n",
    "The good model is the one giving a samll cross entropy (ln). Problem goes from maximizing prob to minimize cross entropy.\n",
    "\n",
    "Cross entropy: if I have a bunch of events and a bunch of probabilities, how likely is that those events happen based on the probabilities.if it very likely then we have a small cross entropy, if it unlikely then we have a large cross entropy.\n",
    "\n",
    "The forumla encompasses the sums of the negatives of lagorithms which is precisely the cross-entropy. So the cross-entropy really tells us when two vectors are similar or different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43386458262986227"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return - np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "\n",
    "cross_entropy(np.array([1, 1, 0]), np.array([0.9, 0.8, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
